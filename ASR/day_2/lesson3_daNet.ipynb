{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 3\n",
    "\n",
    "В данном уроке мы попробуем запустить наше DTW уже на реальном звуке. Для начала это будут простые слова \"да\" и \"нет\". Имеется небольшая база с файлами различных вариаций произнесения этих слов. Часть из них (эталоны) будут использованны для построения графа. Для остальных же (записей) будет поочередно запущен DTW алгорим для определения ближайшего к ним файла из эталонов.\n",
    "\n",
    "MFCC признаки мы уже посчитали и сохранили в формате ark в файлах etalons_mfcc.txtftr и records_mfcc.txtftr соответственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее, наш граф был способен идти только вровень, либо сжимать запись (оставаться в том же узле графа) относительно кадров эталона. Но необходимо еще уметь и растягивать запись. Для этого нужно ввести дополнительные переходы через один и два состояния для узлов графа.\n",
    "\n",
    "<br>\n",
    "<img src=\"graph.png\">\n",
    "<br>\n",
    "\n",
    "<b>Задание 1:</b>\n",
    "Добавить для узлов графа дополнительные переходы через один и два состояния (нулевой узел должен остаться прежним)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import FtrFile\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, ftr, idx):\n",
    "        self.ftr = ftr           # вектор признаков узла \n",
    "        self.isFinal = False     # является ли этот узел финальнвм в слове\n",
    "        self.word = None         # слово эталона (назначается только для финального узла)       \n",
    "        self.nextStates = []     # список следующих узлов\n",
    "        self.idx = idx           # индекс узла\n",
    "        self.bestToken = None    # лучший токен (по минимуму дистанции) в узле\n",
    "        self.currentWord = None  # текущее слово эталона\n",
    "        \n",
    "        \n",
    "def load_graph(rxfilename):\n",
    "    startState = State(None, 0)\n",
    "    graph = [startState, ]\n",
    "    stateIdx = 1\n",
    "    for word, features in FtrFile.FtrDirectoryReader(rxfilename):\n",
    "        prevState = startState\n",
    "        for frame in range(features.nSamples):\n",
    "            state = State(features.readvec(), stateIdx)\n",
    "            state.currentWord = word           # слово эталона теперь будет храниться в каждом узле\n",
    "            state.nextStates.append(state)          \n",
    "            prevState.nextStates.append(state)\n",
    "            prevState = state\n",
    "            \n",
    "            #-----------------------------TODO-----------------------------------\n",
    "            \n",
    "            #--------------------------------------------------------------------\n",
    "            stateIdx += 1\n",
    "            \n",
    "        if state:\n",
    "            state.word = word\n",
    "            state.isFinal = True\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий блок кода проверяет ваш граф на некоторые ключевые параметры и записывает в удобном для чтения виде в файл graph.txt. Сравните его с заведомо правильным графом, сохраненным в graph_reference.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SEE graph.txt ***\n",
      "*** END DEBUG. GRAPH ***\n"
     ]
    }
   ],
   "source": [
    "def check_graph(graph):\n",
    "    assert len(graph) > 0, \"graph is empty.\"\n",
    "    assert graph[0].ftr is None \\\n",
    "        and graph[0].word is None \\\n",
    "        and not graph[0].isFinal, \"broken start state in graph.\"\n",
    "    idx = 0\n",
    "    for state in graph:\n",
    "        assert state.idx == idx\n",
    "        idx += 1\n",
    "        assert (state.isFinal and state.word is not None) \\\n",
    "            or (not state.isFinal and state.word is None)\n",
    "\n",
    "\n",
    "def print_graph(graph):\n",
    "    with open('graph.txt', 'w') as fn:\n",
    "        np.set_printoptions(formatter={'float': '{: 0.1f}'.format})\n",
    "        for state in graph:\n",
    "            nextStatesIdxs = [s.idx for s in state.nextStates]\n",
    "            fn.write(\"State: idx={} word={} isFinal={} nextStatesIdxs={} ftr={} \\n\".format(\n",
    "                state.idx, state.word, state.isFinal, nextStatesIdxs, state.ftr))\n",
    "    print(\"*** SEE graph.txt ***\")\n",
    "    print(\"*** END DEBUG. GRAPH ***\")\n",
    "\n",
    "    \n",
    "etalons = \"ark,t:etalons_mfcc.txtftr\"\n",
    "graph = load_graph(etalons)\n",
    "check_graph(graph)\n",
    "# Сохранить граф в читабельном виде в файл graph.txt:\n",
    "print_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализованный в прошлом уроке TPA в данном случае будет перебирать все возможные варианты разметки, что приведет к значительному увеличению времени работы нашего DTW. Для решения этой проблемы мы будем отбрасывать \"ненужные\" токены еще на этапе прохождения по графу. Этим занимаются, так называемые, beam и state prunings.\n",
    "\n",
    "### state pruning:\n",
    "В классе State нужно добавить атрибут best_token – ссылку на лучший токен, заканчивающийся в данном стейте на данном кадре записи. После порождения всех токенов за текущий кадр записи, пройдемся по каждому из полученных nextTokens, затем впишем текущий токен в State.best_token (здесь State – это узел, на котором закончился токен), убив предыдущий лучший токен, либо убьем сам токен, если он хуже лучшего на этом узле. За жизнеспособность токена отвечает его атрибут is_alive: True или False соответственно.\n",
    "\n",
    "После этого необходимо очистить поле best_token у всех узлов графа.\n",
    "\n",
    "### beam pruning:\n",
    "Идея состоит в том, чтобы на каждом кадре записи находить плохие токены и откидывать их (token.is_alive = False). \n",
    "Плохие – это,очевидно, накопившие слишком большое отклонение от стейтов,по которым они идут. Слишком большое отклонение – это непонятно какое (может токен плохой, может слово слишком длинное, может звук очень плохой – не разобрать).\n",
    "\n",
    "Поэтому плохость токена считают относительно лучшего токена. Заведем переменную thr_common (обычно её называют beam – ширина луча поиска; у нас это common threshold – “общий порог” – по историческим причинам). И если token.dist > best_token.dist + thr_common, то token плохой и мы его отбросим.\n",
    "\n",
    "Выкидывая какой-то токен из-за его отклонеиня, мы рискуем тем, что через сколько-то кадров все потомки выживших токенов могут оказаться очень плохими, а только потомки отброшенного токена оказались бы чудо как хороши. То есть, вводя thr_common, мы вводим ошибку.\n",
    "Поэтому thr_common нужно подобрать так, чтобы скорость сильно выросла, а ошибка выросла незначительно.\n",
    "\n",
    "<br>\n",
    "Введение этих методов может привести к тому, что у нас просто не окажется в конце выживших токенов в финальных узлах графа. Для того, чтобы иметь возможность выдавать результат в этом случае, мы введем дополнительный атрибут currentWord у класса State. Теперь в любом узле каждой ветви будет храниться слово соответствующего эталона для этой ветви. \n",
    "\n",
    "Тогда в конце работы DTW, если у нас не будет живых финальных токенов, то мы просто выберем лучший из оставшихся и по полю state.currentWord определим слово эталона.\n",
    "\n",
    "<b>Задание 2:</b> \n",
    "- Написать функцию findBest для поиска токена с минимальной дистанцией.\n",
    "- Реализовать функции для state и beam pruning (здесь нам и может пригодиться функция findBest).\n",
    "- Разобраться с вычислением WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, state, dist=0.0, sentence=\"\"):\n",
    "        self.state = state\n",
    "        self.dist = dist\n",
    "        self.sentence = sentence\n",
    "        self.alive = True\n",
    "    \n",
    "\n",
    "def findBest(Tokens):\n",
    "    #---------------------------------------TODO---------------------------------  \n",
    "    \n",
    "    #-----------------------------------------------------------------------------\n",
    "    return bestToken\n",
    "\n",
    "\n",
    "def beamPruning(nextTokens):\n",
    "    thr_common = 70 # можно менять\n",
    "    #--------------------------------TODO--------------------------------------\n",
    "    # 1. Ищем лучший токен из nextTokens с помощью findBest\n",
    "    # 2. Присваиваем token.aliv значение False, если дистанция этого токена больше, чем\n",
    "    #    длина лучшего токена + thr_common\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    return nextTokens\n",
    "\n",
    "\n",
    "def statePruning(nextTokens):\n",
    "    for i in range(len(nextTokens)):\n",
    "        #--------------------------TODO---------------------------------\n",
    "        #\n",
    "        #\n",
    "        #---------------------------------------------------------------\n",
    "        \n",
    "    # Сбросить bestToken на None для всеx узлов графа\n",
    "    \n",
    "    #-------------------------------------------------------------------\n",
    "    return nextTokens\n",
    "\n",
    "\n",
    "def distance(X, Y):\n",
    "    result = float(np.sqrt(sum(pow(X - Y, 2))))\n",
    "    return result\n",
    "\n",
    "\n",
    "def recognize(features, graph, rec_results):\n",
    "\n",
    "    print(\"-\" * 23)\n",
    "    startTime = time.time()\n",
    "    startState = graph[0]\n",
    "    activeTokens = [Token(startState), ]\n",
    "    nextTokens = []\n",
    "\n",
    "    for frame in range(features.nSamples):\n",
    "        ftrCurrentFrameRecord = features.readvec()\n",
    "        for token in activeTokens:\n",
    "            if token.alive:\n",
    "                for transitionState in token.state.nextStates:\n",
    "                    newToken = Token(transitionState, token.dist, token.sentence)\n",
    "                    newToken.dist += distance(ftrCurrentFrameRecord, transitionState.ftr)\n",
    "                    nextTokens.append(newToken)\n",
    "        # state and beam prunings:\n",
    "        nextTokens = statePruning(nextTokens)         \n",
    "        nextTokens = beamPruning(nextTokens) \n",
    "\n",
    "        activeTokens = nextTokens\n",
    "        nextTokens = []                                    \n",
    "        \n",
    "    # поиск финальных токенов:\n",
    "    finalTokens = []\n",
    "    for token in activeTokens:\n",
    "        if token.state.isFinal and token.alive:\n",
    "            finalTokens.append(token)\n",
    "\n",
    "    # если нет финальных, то берем лучший из выживших:\n",
    "    if len(finalTokens) != 0:\n",
    "        winToken = findBest(finalTokens)\n",
    "    else:\n",
    "        winToken = findBest(activeTokens)\n",
    "        winToken.state.word = winToken.state.currentWord\n",
    "\n",
    "    # вывод результата DTW\n",
    "    print(\"result: {} ==> {}\".format(filename, winToken.state.word))\n",
    "    endTime = time.time()\n",
    "    print(\"time: {} sec\".format(round(endTime-startTime, 2)))\n",
    "\n",
    "    # совпадает ли запись с полученным эталоном:  \n",
    "    record_word = filename.split('_')[0]\n",
    "    etalon_word = winToken.state.word.split('_')[0]\n",
    "    rec_results.append(etalon_word == record_word)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запустим нашу программу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "result: da_06 ==> da_02\n",
      "time: 2.37 sec\n",
      "-----------------------\n",
      "result: da_07 ==> da_02\n",
      "time: 5.09 sec\n",
      "-----------------------\n",
      "result: da_08 ==> da_04\n",
      "time: 3.58 sec\n",
      "-----------------------\n",
      "result: da_09 ==> da_02\n",
      "time: 3.44 sec\n",
      "-----------------------\n",
      "result: da_10 ==> da_04\n",
      "time: 4.56 sec\n",
      "-----------------------\n",
      "result: da_11 ==> da_02\n",
      "time: 4.17 sec\n",
      "-----------------------\n",
      "result: da_12 ==> da_05\n",
      "time: 3.12 sec\n",
      "-----------------------\n",
      "result: da_13 ==> da_02\n",
      "time: 3.02 sec\n",
      "-----------------------\n",
      "result: da_14 ==> da_02\n",
      "time: 2.81 sec\n",
      "-----------------------\n",
      "result: da_15 ==> da_02\n",
      "time: 5.24 sec\n",
      "-----------------------\n",
      "result: da_16 ==> da_02\n",
      "time: 2.38 sec\n",
      "-----------------------\n",
      "result: da_17 ==> da_05\n",
      "time: 4.06 sec\n",
      "-----------------------\n",
      "result: da_18 ==> da_05\n",
      "time: 2.15 sec\n",
      "-----------------------\n",
      "result: da_19 ==> da_05\n",
      "time: 4.04 sec\n",
      "-----------------------\n",
      "result: net_06 ==> da_02\n",
      "time: 3.12 sec\n",
      "-----------------------\n",
      "result: net_07 ==> net_04\n",
      "time: 3.15 sec\n",
      "-----------------------\n",
      "result: net_08 ==> net_02\n",
      "time: 3.03 sec\n",
      "-----------------------\n",
      "result: net_09 ==> net_04\n",
      "time: 3.68 sec\n",
      "-----------------------\n",
      "result: net_10 ==> net_03\n",
      "time: 2.44 sec\n",
      "-----------------------\n",
      "result: net_11 ==> net_03\n",
      "time: 3.72 sec\n",
      "-----------------------\n",
      "result: net_12 ==> net_04\n",
      "time: 3.21 sec\n",
      "-----------------------\n",
      "result: net_13 ==> net_04\n",
      "time: 3.15 sec\n",
      "-----------------------\n",
      "result: net_14 ==> net_04\n",
      "time: 5.45 sec\n",
      "-----------------------\n",
      "result: net_15 ==> net_04\n",
      "time: 4.33 sec\n",
      "-----------------------\n",
      "result: net_16 ==> net_04\n",
      "time: 4.14 sec\n",
      "-----------------------\n",
      "result: net_17 ==> net_05\n",
      "time: 4.11 sec\n",
      "-----------------------\n",
      "result: net_18 ==> net_04\n",
      "time: 3.21 sec\n",
      "-----------------------\n",
      "result: net_19 ==> net_04\n",
      "time: 3.37 sec\n",
      "-----------------------\n",
      "result: net_20 ==> net_04\n",
      "time: 4.14 sec\n",
      "-----------------------\n",
      "result: net_21 ==> net_04\n",
      "time: 3.98 sec\n",
      "-----------------------\n",
      "WER is: 0.033\n",
      "Total time: 1 min 48 sec\n",
      "RTF is: 1.31\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "etalons = \"ark,t:etalons_mfcc.txtftr\"\n",
    "records = \"ark,t:records_mfcc.txtftr\"\n",
    "\n",
    "rec_results = []  # переменная для подсчета точности распознавания\n",
    "\n",
    "s_time = time.time()\n",
    "numbFrame = 0     # счетчик общего количества кадров для расчета RTF\n",
    "\n",
    "graph = load_graph(etalons)\n",
    "\n",
    "for filename, features in FtrFile.FtrDirectoryReader(records):\n",
    "    frame = recognize(features, graph, rec_results)\n",
    "    numbFrame += frame\n",
    "\n",
    "print(\"-\" * 23)\n",
    "print(\"WER is: {}\".format(round(1 - sum(rec_results)/len(rec_results), 3)))\n",
    "e_time = time.time()\n",
    "time = e_time-s_time\n",
    "minut = int(time/60)\n",
    "second = int(time-minut*60)\n",
    "print(\"Total time: {} min {} sec\".format(minut, second))\n",
    "rtf = round(time/(numbFrame*0.01), 2)\n",
    "print(\"RTF is: {}\".format(rtf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Задание 3:</b> Подбирите значение порога thr_common и количество дополнительных переходов для узлов так, чтобы получить минимально возможно значение WER для данной базы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
